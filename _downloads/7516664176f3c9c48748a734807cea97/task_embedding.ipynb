{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Embedding\n\nIn AgentScope, the embedding module provides a unified interface for vector representation generation, which features:\n\n- Support **caching embeddings** to avoid redundant API calls\n- Support **multiple embedding providers** with a consistent API\n\nAgentScope has built-in embedding classes for the following API providers:\n\n.. list-table::\n    :header-rows: 1\n\n    * - Provider\n      - Class\n    * - OpenAI\n      - ``OpenAITextEmbedding``\n    * - Gemini\n      - ``GeminiTextEmbedding``\n    * - DashScope\n      - ``DashScopeTextEmbedding``, ``DashScopeMultiModalEmbedding``\n    * - Ollama\n      - ``OllamaTextEmbedding``\n\nAll classes inherit from ``EmbeddingModelBase``, implementing the ``__call__`` method and generating ``EmbeddingResponse`` object with the embeddings and usage information.\nThe ``DashScopeMultiModalEmbedding`` supports multi-modal embeddings for text, images, and videos.\n\nTaking the DashScope embedding class as an example, you can use it as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport os\nimport tempfile\n\nfrom agentscope.embedding import DashScopeTextEmbedding, FileEmbeddingCache\n\n\nasync def example_dashscope_embedding() -> None:\n    \"\"\"Example usage of DashScope text embedding.\"\"\"\n    texts = [\n        \"What is the capital of France?\",\n        \"Paris is the capital city of France.\",\n    ]\n\n    # Initialize the DashScope text embedding instance\n    embedding_model = DashScopeTextEmbedding(\n        model_name=\"text-embedding-v2\",\n        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    )\n\n    # Get the embedding from the model\n    response = await embedding_model(texts)\n\n    print(\"The embedding ID: \", response.id)\n    print(\"The embedding create at: \", response.created_at)\n    print(\"The embedding usage: \", response.usage)\n    print(\"The embedding:\")\n    print(response.embeddings)\n\n\nasyncio.run(example_dashscope_embedding())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can customize your embedding model by subclassing ``EmbeddingModelBase`` and implementing the ``__call__`` method.\n\n## Embedding Cache\nAgentScope provides a base class ``EmbeddingCacheBase`` for caching embeddings, as well as a file-based implementation ``FileEmbeddingCache``.\nIt works as follows in the embedding module:\n\n<img src=\"file://../../_static/images/embedding_cache.png\" align=\"center\" width=\"90%\">\n\nTo use caching, just pass an instance of ``FileEmbeddingCache`` (or your custom cache) to the embedding model's constructor as follows:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_embedding_cache() -> None:\n    \"\"\"Demonstrate embedding with cache functionality.\"\"\"\n    # Example texts\n    texts = [\n        \"What is the capital of France?\",\n        \"Paris is the capital city of France.\",\n    ]\n\n    # Create a temporary directory for cache demonstration\n    # In real applications, you might want to use a persistent directory\n    cache_dir = tempfile.mkdtemp(prefix=\"embedding_cache_\")\n    print(f\"Using cache directory: {cache_dir}\")\n\n    # Initialize the embedding model with cache\n    # We limit the cache to 100 files and 10MB for demonstration purposes\n    embedder = DashScopeTextEmbedding(\n        model_name=\"text-embedding-v3\",\n        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n        embedding_cache=FileEmbeddingCache(\n            cache_dir=cache_dir,\n            max_file_number=100,\n            max_cache_size=10,  # Maximum cache size in MB\n        ),\n    )\n\n    # First call - will fetch from API and store in cache\n    print(\"\\n=== First API Call (No Cache Hit) ===\")\n    start_time = asyncio.get_event_loop().time()\n    response1 = await embedder(texts)\n    elapsed_time1 = asyncio.get_event_loop().time() - start_time\n    print(f\"Source: {response1.source}\")  # Should be 'api'\n    print(f\"Time taken: {elapsed_time1:.4f} seconds\")\n    print(f\"Tokens used: {response1.usage.tokens}\")\n\n    # Second call with the same texts - should use cache\n    print(\"\\n=== Second API Call (Cache Hit Expected) ===\")\n    start_time = asyncio.get_event_loop().time()\n    response2 = await embedder(texts)\n    elapsed_time2 = asyncio.get_event_loop().time() - start_time\n    print(f\"Source: {response2.source}\")  # Should be 'cache'\n    print(f\"Time taken: {elapsed_time2:.4f} seconds\")\n    print(\n        f\"Tokens used: {response2.usage.tokens}\",\n    )  # Should be 0 for cached results\n    print(\n        f\"Speed improvement: {elapsed_time1 / elapsed_time2:.1f}x faster with cache\",\n    )\n\n\nasyncio.run(example_embedding_cache())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}