{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TTS\n\nAgentScope provides a unified interface for Text-to-Speech (TTS) models across multiple API providers.\nThis tutorial demonstrates how to use TTS models in AgentScope.\n\nAgentScope supports the following TTS APIs:\n\n.. list-table:: Built-in TTS Models\n    :header-rows: 1\n\n    * - API\n      - Class\n      - Streaming Input\n      - Non-Streaming Input\n      - Streaming Output\n      - Non-Streaming Output\n    * - DashScope Realtime API\n      - ``DashScopeRealtimeTTSModel``\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n    * - DashScope API\n      - ``DashScopeTTSModel``\n      - \u274c\n      - \u2705\n      - \u2705\n      - \u2705\n    * - OpenAI API\n      - ``OpenAITTSModel``\n      - \u274c\n      - \u2705\n      - \u2705\n      - \u2705\n    * - Gemini API\n      - ``GeminiTTSModel``\n      - \u274c\n      - \u2705\n      - \u2705\n      - \u2705\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The streaming input and output in AgentScope TTS models are all accumulative.</p></div>\n\n**Choosing the Right Model:**\n\n- **Use Non-Realtime TTS** when you have complete text ready (e.g., pre-written\n  responses, complete LLM outputs)\n- **Use Realtime TTS** when text is generated progressively (e.g., streaming\n  LLM responses) for lower latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport os\n\nfrom agentscope.agent import ReActAgent, UserAgent\nfrom agentscope.formatter import DashScopeChatFormatter\nfrom agentscope.message import Msg\nfrom agentscope.model import DashScopeChatModel\nfrom agentscope.tts import (\n    DashScopeRealtimeTTSModel,\n    DashScopeTTSModel,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Realtime TTS\nNon-realtime TTS models process complete text inputs and are the simplest\nto use. You can directly call their ``synthesize()`` method.\n\nTaking DashScope TTS model as an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_non_realtime_tts() -> None:\n    \"\"\"A basic example of using non-realtime TTS models.\"\"\"\n    # Example with DashScope TTS\n    tts_model = DashScopeTTSModel(\n        api_key=os.environ.get(\"DASHSCOPE_API_KEY\", \"\"),\n        model_name=\"qwen3-tts-flash\",\n        voice=\"Cherry\",\n        stream=False,  # Non-streaming output\n    )\n\n    msg = Msg(\n        name=\"assistant\",\n        content=\"Hello, this is DashScope TTS.\",\n        role=\"assistant\",\n    )\n\n    # Directly synthesize without connecting\n    tts_response = await tts_model.synthesize(msg)\n\n    # tts_response.content contains an audio block with base64-encoded audio data\n    print(\n        \"The length of audio data:\",\n        len(tts_response.content[\"source\"][\"data\"]),\n    )\n\n\nasyncio.run(example_non_realtime_tts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Streaming Output for Lower Latency:**\n\nWhen ``stream=True``, the model returns audio chunks progressively, allowing\nyou to start playback before synthesis completes. This reduces perceived latency.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_non_realtime_tts_streaming() -> None:\n    \"\"\"An example of using non-realtime TTS models with streaming output.\"\"\"\n    # Example with DashScope TTS with streaming output\n    tts_model = DashScopeTTSModel(\n        api_key=os.environ.get(\"DASHSCOPE_API_KEY\", \"\"),\n        model_name=\"qwen3-tts-flash\",\n        voice=\"Cherry\",\n        stream=True,  # Enable streaming output\n    )\n\n    msg = Msg(\n        name=\"assistant\",\n        content=\"Hello, this is DashScope TTS with streaming output.\",\n        role=\"assistant\",\n    )\n\n    # Synthesize and receive an async generator for streaming output\n    async for tts_response in await tts_model.synthesize(msg):\n        # Process each audio chunk as it arrives\n        print(\n            \"Received audio chunk of length:\",\n            len(tts_response.content[\"source\"][\"data\"]),\n        )\n\n\nasyncio.run(example_non_realtime_tts_streaming())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Realtime TTS\nRealtime TTS models are designed for scenarios where text is generated\nincrementally, such as streaming LLM responses. This enables the lowest\npossible latency by starting audio synthesis before the complete text is ready.\n\n**Key Concepts:**\n\n- **Stateful Processing**: Realtime TTS maintains state for a single streaming\n  session, identified by ``msg.id``. Only one streaming session can be active\n  at a time.\n- **Two Methods**:\n\n  - ``push(msg)``: Non-blocking method that submits text chunks and returns\n    immediately. May return partial audio if available.\n  - ``synthesize(msg)``: Blocking method that finalizes the session and returns\n    all remaining audio. When ``stream=True``, it returns an async generator.\n\n```python\nasync def example_realtime_tts_streaming():\n    tts_model = DashScopeRealtimeTTSModel(\n        api_key=os.environ.get(\"DASHSCOPE_API_KEY\", \"\"),\n        model_name=\"qwen3-tts-flash-realtime\",\n        voice=\"Cherry\",\n        stream=False,\n    )\n\n    # realtime tts model received accumulative text chunks\n    res = await tts_model.push(msg_chunk_1)  # non-blocking\n    res = await tts_model.push(msg_chunk_2)  # non-blocking\n    ...\n    res = await tts_model.synthesize(final_msg)  # blocking, get all remaining audio\n```\nWhen setting ``stream=True`` during initialization, the ``synthesize()`` method returns an async generator of ``TTSResponse`` objects, allowing you to process audio chunks as they arrive.\n\n\n## Integrating with ReActAgent\nAgentScope agents can automatically synthesize their responses to speech\nwhen provided with a TTS model. This works seamlessly with both realtime\nand non-realtime TTS models.\n\n**How It Works:**\n\n1. The agent generates a text response (potentially streamed from an LLM)\n2. The TTS model synthesizes the text to audio automatically\n3. The synthesized audio is attached to the ``speech`` field of the ``Msg`` object\n4. The audio is played during the agent's ``self.print()`` method\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_agent_with_tts() -> None:\n    \"\"\"An example of using TTS with ReActAgent.\"\"\"\n    # Create an agent with TTS enabled\n    agent = ReActAgent(\n        name=\"Assistant\",\n        sys_prompt=\"You are a helpful assistant.\",\n        model=DashScopeChatModel(\n            api_key=os.environ.get(\"DASHSCOPE_API_KEY\", \"\"),\n            model_name=\"qwen-max\",\n            stream=True,\n        ),\n        formatter=DashScopeChatFormatter(),\n        # Enable TTS\n        tts_model=DashScopeRealtimeTTSModel(\n            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n            model_name=\"qwen3-tts-flash-realtime\",\n            voice=\"Cherry\",\n        ),\n    )\n    user = UserAgent(\"User\")\n\n    # Build a conversation just like normal\n    msg = None\n    while True:\n        msg = await agent(msg)\n        msg = await user(msg)\n        if msg.get_text_content() == \"exit\":\n            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customizing TTS Model\nYou can create custom TTS implementations by inheriting from ``TTSModelBase``.\nThe base class provides a flexible interface for both realtime and non-realtime\nTTS models.\nWe use an attribute ``supports_streaming_input`` to indicate if the TTS model is realtime or not.\n\nFor realtime TTS models, you need to implement the ``connect``, ``close``, ``push`` and ``synthesize`` methods to handle the lifecycle and streaming input.\n\nWhile for non-realtime TTS models, you only need to implement the ``synthesize`` method.\n\n## Further Reading\n- `agent` - Learn more about agents in AgentScope\n- `message` - Understand message format in AgentScope\n- API Reference: :class:`agentscope.tts.TTSModelBase`\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}