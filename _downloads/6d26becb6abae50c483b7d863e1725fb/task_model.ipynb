{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Model\n\nIn this tutorial, we introduce the model APIs integrated in AgentScope, how to use them and how to integrate new model APIs.\nThe supported model APIs and providers include:\n\n.. list-table::\n    :header-rows: 1\n\n    * - API\n      - Class\n      - Compatible\n      - Streaming\n      - Tools\n      - Vision\n      - Reasoning\n    * - OpenAI\n      - ``OpenAIChatModel``\n      - vLLM, DeepSeek\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n    * - DashScope\n      - ``DashScopeChatModel``\n      -\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n    * - Anthropic\n      - ``AnthropicChatModel``\n      -\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n    * - Gemini\n      - ``GeminiChatModel``\n      -\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n    * - Ollama\n      - ``OllamaChatModel``\n      -\n      - \u2705\n      - \u2705\n      - \u2705\n      - \u2705\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>When using vLLM, you need to configure the appropriate tool calling parameters for different models during deployment, such as ``--enable-auto-tool-choice``, ``--tool-call-parser``, etc. For more details, refer to the [official vLLM documentation](https://docs.vllm.ai/en/latest/features/tool_calling.html).</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For OpenAI-compatible models (e.g. vLLM, Deepseek), developers can use the ``OpenAIChatModel`` class, and specify the API endpoint by the ``client_args`` parameter: ``client_args={\"base_url\": \"http://your-api-endpoint\"}``. For example:\n\n```python\nOpenAIChatModel(client_args={\"base_url\": \"http://localhost:8000/v1\"})</p></div>\n```\n<div class=\"alert alert-info\"><h4>Note</h4><p>Model behavior parameters (such as temperature, maximum length, etc.) can be preset in the constructor function via the ``generate_kwargs`` parameter. For example:\n\n```python\nOpenAIChatModel(generate_kwargs={\"temperature\": 0.3, \"max_tokens\": 1000})</p></div>\n```\nTo provide unified model interfaces, the above model classes has the following common methods:\n\n- The first three arguments of the ``__call__`` method are ``messages`` , ``tools`` and ``tool_choice``, representing the input messages, JSON schema of tool functions, and tool selection mode, respectively.\n- The return type are either a ``ChatResponse`` instance or an async generator of ``ChatResponse`` in streaming mode.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Different model APIs differ in the input message format, refer to `prompt` for more details.</p></div>\n\nThe ``ChatResponse`` instance contains the generated thinking/text/tool use content, identity, created time and usage information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport json\nimport os\n\nfrom agentscope.message import TextBlock, ToolUseBlock, ThinkingBlock, Msg\nfrom agentscope.model import ChatResponse, DashScopeChatModel\n\nresponse = ChatResponse(\n    content=[\n        ThinkingBlock(\n            type=\"thinking\",\n            thinking=\"I should search for AgentScope on Google.\",\n        ),\n        TextBlock(type=\"text\", text=\"I'll search for AgentScope on Google.\"),\n        ToolUseBlock(\n            type=\"tool_use\",\n            id=\"642n298gjna\",\n            name=\"google_search\",\n            input={\"query\": \"AgentScope?\"},\n        ),\n    ],\n)\n\nprint(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking ``DashScopeChatModel`` as an example, we can use it to create a chat model instance and call it with messages and tools:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_model_call() -> None:\n    \"\"\"An example of using the DashScopeChatModel.\"\"\"\n    model = DashScopeChatModel(\n        model_name=\"qwen-max\",\n        api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n        stream=False,\n    )\n\n    res = await model(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hi!\"},\n        ],\n    )\n\n    # You can directly create a ``Msg`` object with the response content\n    msg_res = Msg(\"Friday\", res.content, \"assistant\")\n\n    print(\"The response:\", res)\n    print(\"The response as Msg:\", msg_res)\n\n\nasyncio.run(example_model_call())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming\nTo enable streaming model, set the ``stream`` parameter in the model constructor to ``True``.\nWhen streaming is enabled, the ``__call__`` method will return an **async generator** that yields ``ChatResponse`` instances as they are generated by the model.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The streaming mode in AgentScope is designed to be **cumulative**, meaning the content in each chunk contains all the previous content plus the newly generated content.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_streaming() -> None:\n    \"\"\"An example of using the streaming model.\"\"\"\n    model = DashScopeChatModel(\n        model_name=\"qwen-max\",\n        api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n        stream=True,\n    )\n\n    generator = await model(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Count from 1 to 20, and just report the number without any other information.\",\n            },\n        ],\n    )\n    print(\"The type of the response:\", type(generator))\n\n    i = 0\n    async for chunk in generator:\n        print(f\"Chunk {i}\")\n        print(f\"\\ttype: {type(chunk.content)}\")\n        print(f\"\\t{chunk}\\n\")\n        i += 1\n\n\nasyncio.run(example_streaming())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reasoning\nAgentScope supports reasoning models by providing the ``ThinkingBlock``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def example_reasoning() -> None:\n    \"\"\"An example of using the reasoning model.\"\"\"\n    model = DashScopeChatModel(\n        model_name=\"qwen-turbo\",\n        api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n        enable_thinking=True,\n    )\n\n    res = await model(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Who am I?\"},\n        ],\n    )\n\n    last_chunk = None\n    async for chunk in res:\n        last_chunk = chunk\n    print(\"The final response:\")\n    print(last_chunk)\n\n\nasyncio.run(example_reasoning())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tools API\nDifferent model providers differ in their tools APIs, e.g. the tools JSON schema, the tool call/response format.\nTo provide a unified interface, AgentScope solves the problem by:\n\n- Providing unified tool call block `ToolUseBlock <tool-block>` and tool response block `ToolResultBlock <tool-block>`, respectively.\n- Providing a unified tools interface in the ``__call__`` method of the model classes, that accepts a list of tools JSON schemas as follows:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "json_schemas = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"google_search\",\n            \"description\": \"Search for a query on Google.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query.\",\n                    },\n                },\n                \"required\": [\"query\"],\n            },\n        },\n    },\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n- `message`\n- `prompt`\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}